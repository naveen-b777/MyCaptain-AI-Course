{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import sys\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import tensorflow\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout, LSTM\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "file= open('frankenstein.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenisation and standardisation\n",
    "def tokenize_words(input):\n",
    "    input=input.lower()\n",
    "    tokenizer= RegexpTokenizer(r'\\w+')\n",
    "    tokens= tokenizer.tokenize(input)\n",
    "    filtered= filter(lambda token:token not in stopwords.words('english'),tokens)\n",
    "    return \"\".join(filtered)\n",
    "    \n",
    "processed_inputs= tokenize_words(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#characters to numbers\n",
    "chars= sorted(list(set(processed_inputs)))\n",
    "char_to_num= dict((c,i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 220857\n",
      "Total vocab: 42\n"
     ]
    }
   ],
   "source": [
    "#check if words to chars or chars to num has worked\n",
    "input_len= len(processed_inputs)\n",
    "vocab_len= len(chars)\n",
    "print('Total number of characters:',input_len)\n",
    "print('Total vocab:',vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seq length\n",
    "seq_length=100\n",
    "x_data=[]\n",
    "y_data=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patterns: 220757\n"
     ]
    }
   ],
   "source": [
    "#looping through the sequence\n",
    "for i in range(0,input_len-seq_length,1):\n",
    "    in_seq= processed_inputs[i:i+seq_length]\n",
    "    out_seq=processed_inputs[i+seq_length]\n",
    "    x_data.append([char_to_num[char] for char in in_seq])\n",
    "    y_data.append(char_to_num[out_seq])\n",
    "\n",
    "n_patterns= len(x_data)\n",
    "print('Total patterns:',n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting input seq into numpy array\n",
    "x= numpy.reshape(x_data,(n_patterns,seq_length,1))\n",
    "x= x/float(vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding\n",
    "y= np_utils.to_categorical(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256,input_shape=(x.shape[1],x.shape[2]),return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256,return_sequences=True))\n",
    "model.add(Dropout(0.2))          \n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1],activation='softmax'))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compiling the model\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving weights\n",
    "filepath= 'model_weights_saved.hdf5'\n",
    "checkpoint= ModelCheckpoint(filepath, monitor='loss',verbose=1,save_best_only=True,mode='min')\n",
    "desired_callbacks= [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "863/863 [==============================] - ETA: 0s - loss: 2.9301\n",
      "Epoch 00001: loss improved from inf to 2.93011, saving model to model_weights_saved.hdf5\n",
      "863/863 [==============================] - 131s 152ms/step - loss: 2.9301\n",
      "Epoch 2/4\n",
      "863/863 [==============================] - ETA: 0s - loss: 2.9079\n",
      "Epoch 00002: loss improved from 2.93011 to 2.90790, saving model to model_weights_saved.hdf5\n",
      "863/863 [==============================] - 131s 152ms/step - loss: 2.9079\n",
      "Epoch 3/4\n",
      "863/863 [==============================] - ETA: 0s - loss: 2.8964\n",
      "Epoch 00003: loss improved from 2.90790 to 2.89644, saving model to model_weights_saved.hdf5\n",
      "863/863 [==============================] - 132s 153ms/step - loss: 2.8964\n",
      "Epoch 4/4\n",
      "863/863 [==============================] - ETA: 0s - loss: 2.8646\n",
      "Epoch 00004: loss improved from 2.89644 to 2.86463, saving model to model_weights_saved.hdf5\n",
      "863/863 [==============================] - 133s 154ms/step - loss: 2.8646\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f201119fe10>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting model and waiting for it to get trained\n",
    "model.fit(x,y,epochs=4,batch_size=256,callbacks=desired_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recompiling the model with saved weights\n",
    "filename= 'model_weights_saved.hdf5'\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output of the model back to character\n",
    "num_to_char= dict((i,c) for i,c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:\n",
      "\" ndencephilosopherscountrywhoseknowledgediscoveriesindispensableusepresentundertakinglattermethodobta \"\n"
     ]
    }
   ],
   "source": [
    "# random seed to help in generation\n",
    "start= numpy.random.randint(0,len(x_data)-1)\n",
    "pattern= x_data[start]\n",
    "print('Random Seed:')\n",
    "print(\"\\\"\", ''.join([num_to_char[value] for value in pattern]),\"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reeneeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee"
     ]
    }
   ],
   "source": [
    "# generating text\n",
    "for i in range(1000):\n",
    "    x= numpy.reshape(pattern, (1,len(pattern),1))\n",
    "    x=x/float(vocab_len)\n",
    "    prediction= model.predict(x,verbose=0)\n",
    "    index= numpy.argmax(prediction)\n",
    "    result= num_to_char[index]\n",
    "    seq_in=[num_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern=pattern[1:len(pattern)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
